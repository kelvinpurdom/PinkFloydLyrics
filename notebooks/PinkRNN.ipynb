{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../raw_data/pink_floyd_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'side': 1,\n",
       " 'that': 2,\n",
       " 'cat’s': 3,\n",
       " 'something': 4,\n",
       " 'i': 5,\n",
       " 'can’t': 6,\n",
       " 'explain': 7,\n",
       " 'cat': 8,\n",
       " 'you’re': 9,\n",
       " 'a': 10,\n",
       " 'the': 11,\n",
       " 'be': 12,\n",
       " 'lucifer': 13,\n",
       " 'always': 14,\n",
       " 'by': 15,\n",
       " 'your': 16,\n",
       " 'around': 17,\n",
       " 'sam': 18,\n",
       " 'siam': 19,\n",
       " 'sitting': 20,\n",
       " 'jennifer': 21,\n",
       " 'gentle': 22,\n",
       " 'witch': 23,\n",
       " 'left': 24,\n",
       " 'he’s': 25,\n",
       " 'right': 26,\n",
       " 'oh': 27,\n",
       " 'no': 28,\n",
       " 'go': 29,\n",
       " 'to': 30,\n",
       " 'sea': 31,\n",
       " 'hip': 32,\n",
       " 'ship’s': 33,\n",
       " 'somewhere': 34,\n",
       " 'anywhere': 35,\n",
       " 'at': 36,\n",
       " 'night': 37,\n",
       " 'prowling': 38,\n",
       " 'sifting': 39,\n",
       " 'sand': 40,\n",
       " 'hiding': 41,\n",
       " 'on': 42,\n",
       " 'ground': 43,\n",
       " 'he’ll': 44,\n",
       " 'found': 45,\n",
       " 'when': 46}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts([df.lyrics.iloc[1].replace('\\n',' \\n ')])\n",
    "tokenizer.word_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.lyrics.iloc[1].split('\\n')\n",
    "text = [re.sub(r'\\d+', '', i) for i in text]\n",
    "corpus = list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Lucifer, go to sea',\n",
       " 'Hiding around on the ground',\n",
       " 'Somewhere, anywhere',\n",
       " 'Be a hip cat, be a ship’s cat',\n",
       " 'At night prowling, sifting sand',\n",
       " 'He’ll be found when you’re around',\n",
       " 'You’re the left side, he’s the right side',\n",
       " 'Lucifer Sam, siam cat',\n",
       " 'Always sitting by your side',\n",
       " 'Oh, no!',\n",
       " 'Jennifer Gentle, you’re a witch',\n",
       " 'That cat’s something I can’t explain',\n",
       " 'Always by your side']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "[]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "[13, 29, 30, 31]\n",
      "--------------------------------------------------\n",
      "[13, 29]\n",
      "[13, 29, 30]\n",
      "[13, 29, 30, 31]\n",
      "--------------------------------------------------\n",
      "[41, 17, 42, 11, 43]\n",
      "--------------------------------------------------\n",
      "[41, 17]\n",
      "[41, 17, 42]\n",
      "[41, 17, 42, 11]\n",
      "[41, 17, 42, 11, 43]\n",
      "--------------------------------------------------\n",
      "[34, 35]\n",
      "--------------------------------------------------\n",
      "[34, 35]\n",
      "--------------------------------------------------\n",
      "[12, 10, 32, 8, 12, 10, 33, 8]\n",
      "--------------------------------------------------\n",
      "[12, 10]\n",
      "[12, 10, 32]\n",
      "[12, 10, 32, 8]\n",
      "[12, 10, 32, 8, 12]\n",
      "[12, 10, 32, 8, 12, 10]\n",
      "[12, 10, 32, 8, 12, 10, 33]\n",
      "[12, 10, 32, 8, 12, 10, 33, 8]\n",
      "--------------------------------------------------\n",
      "[36, 37, 38, 39, 40]\n",
      "--------------------------------------------------\n",
      "[36, 37]\n",
      "[36, 37, 38]\n",
      "[36, 37, 38, 39]\n",
      "[36, 37, 38, 39, 40]\n",
      "--------------------------------------------------\n",
      "[44, 12, 45, 46, 9, 17]\n",
      "--------------------------------------------------\n",
      "[44, 12]\n",
      "[44, 12, 45]\n",
      "[44, 12, 45, 46]\n",
      "[44, 12, 45, 46, 9]\n",
      "[44, 12, 45, 46, 9, 17]\n",
      "--------------------------------------------------\n",
      "[9, 11, 24, 1, 25, 11, 26, 1]\n",
      "--------------------------------------------------\n",
      "[9, 11]\n",
      "[9, 11, 24]\n",
      "[9, 11, 24, 1]\n",
      "[9, 11, 24, 1, 25]\n",
      "[9, 11, 24, 1, 25, 11]\n",
      "[9, 11, 24, 1, 25, 11, 26]\n",
      "[9, 11, 24, 1, 25, 11, 26, 1]\n",
      "--------------------------------------------------\n",
      "[13, 18, 19, 8]\n",
      "--------------------------------------------------\n",
      "[13, 18]\n",
      "[13, 18, 19]\n",
      "[13, 18, 19, 8]\n",
      "--------------------------------------------------\n",
      "[14, 20, 15, 16, 1]\n",
      "--------------------------------------------------\n",
      "[14, 20]\n",
      "[14, 20, 15]\n",
      "[14, 20, 15, 16]\n",
      "[14, 20, 15, 16, 1]\n",
      "--------------------------------------------------\n",
      "[27, 28]\n",
      "--------------------------------------------------\n",
      "[27, 28]\n",
      "--------------------------------------------------\n",
      "[21, 22, 9, 10, 23]\n",
      "--------------------------------------------------\n",
      "[21, 22]\n",
      "[21, 22, 9]\n",
      "[21, 22, 9, 10]\n",
      "[21, 22, 9, 10, 23]\n",
      "--------------------------------------------------\n",
      "[2, 3, 4, 5, 6, 7]\n",
      "--------------------------------------------------\n",
      "[2, 3]\n",
      "[2, 3, 4]\n",
      "[2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[2, 3, 4, 5, 6, 7]\n",
      "--------------------------------------------------\n",
      "[14, 15, 16, 1]\n",
      "--------------------------------------------------\n",
      "[14, 15]\n",
      "[14, 15, 16]\n",
      "[14, 15, 16, 1]\n"
     ]
    }
   ],
   "source": [
    "lines=[]\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    print('--'*25)\n",
    "    print(token_list)\n",
    "    print('--'*25)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        print(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(token_list):\n",
    "  ng = []\n",
    "  for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    ng.append(n_gram_sequence)\n",
    "  return ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Moon in both [houses]...\"...Scorpio, [Arabian Skies], Libra...\"...Pluto was not discovered until 1930...\"\\nLime and limpid green, a second scene\\nA fight between the blue you once knew\\nFloating down, the sound resounds\\nAround the icy waters underground\\nJupiter and Saturn, Oberon, Miranda and Titania\\nNeptune, Titan, stars can frighten\\n\\nBlinding signs flap\\nFlicker, flicker, flicker, blam\\nPow, pow\\nStairway scare Dan Dare who’s there?\\n\\nLime and limpid green, the sound surrounds\\nThe icy waters under\\nLime and limpid green, the sound surrounds\\nThe icy waters underground'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[0]].lyrics.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqform(data):\n",
    "  tokenise = Tokenizer()\n",
    "  input_sequences = []\n",
    "  corpus = []\n",
    "  k=0\n",
    "  for i in range(0,len(df)):\n",
    "      text = df.iloc[[i]].lyrics.iloc[0]\n",
    "      if type(text)==float:\n",
    "          pass\n",
    "      else:\n",
    "          text = text.lower().split(\"\\n\")\n",
    "          text = [re.sub(r'\\d+', '', i) for i in text]\n",
    "          text = list(set(text))\n",
    "          if text==' ':\n",
    "              pass\n",
    "          else:\n",
    "              corpus.extend(text)\n",
    "              k+=1\n",
    "  tokenise.fit_on_texts(corpus)\n",
    "  for line in corpus:\n",
    "      token_list = tokenise.texts_to_sequences([line])[0]\n",
    "      input_sequences.extend(ngram(token_list))\n",
    " \n",
    "  \n",
    "  max_sequence_len = max([len(x) for x in input_sequences])\n",
    "  input_sequences = np.array(pad_sequences(input_sequences,\n",
    "                       maxlen = max_sequence_len, padding='pre'))\n",
    "  \n",
    "  predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "  fin_data = pd.DataFrame(np.hstack((predictors, label.reshape(-1,1))),columns=np.hstack((np.arange(1,predictors.shape[1]+1),np.array(['label']))))\n",
    "  total_words = len(tokenise.word_index) + 1\n",
    "  print('{} number of lyrics inputted'.format(k))\n",
    "\n",
    "  return fin_data,tokenise,max_sequence_len,total_words,predictors,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 number of lyrics inputted\n",
      "(13839, 88) 88 2983\n"
     ]
    }
   ],
   "source": [
    "fdf,tokenise,max_sequence_len,total_words,predictors,label = seqform(df)\n",
    "print(fdf.shape,max_sequence_len,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf .to_csv('fin_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13839\n",
      "13839\n"
     ]
    }
   ],
   "source": [
    "dataX = [fdf.iloc[i,0:87].tolist() for i in range(0,fdf.shape[0])]\n",
    "dataY = [fdf.iloc[i,87] for i in range(0,fdf.shape[0])]\n",
    "print(len(dataX))\n",
    "print(len(dataY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (13839, 87, 1))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 11:48:08.074155: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 87, 150)           447450    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 87, 300)          361200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 87, 300)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1491)              150591    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2983)              4450636   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,570,277\n",
      "Trainable params: 5,570,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 150, input_length=max_sequence_len-1))\n",
    "# Add an LSTM Layer\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))  \n",
    "# A dropout layer for regularisation\n",
    "model.add(Dropout(0.2))\n",
    "# Add another LSTM Layer\n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(total_words/2, activation='relu'))  \n",
    "# In the last layer, the shape should be equal to the total number of words present in our corpus\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "#model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')  #(# Pick a loss function and an optimizer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X,y, epochs= 60,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"weights-improvement-60-0.7534.hdf5\"\n",
    "#model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam',metrics='accuracy')\n",
    "#model.fit(X, y, epochs=20, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('my_model_weights.h5')\n",
    "#model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lyrics(seed_text, next_words):\n",
    "    pred_index=[]\n",
    "    for i in range(next_words):\n",
    "        token_list = tokenise.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=max_sequence_len-1,padding='pre')\n",
    "        #print(token_list.shape)\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len-1, 1))\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index =  np.argmax(predicted)\n",
    "        pred_index.append(predicted_index)\n",
    "        \n",
    "\n",
    "\n",
    "        #predicted_index=1\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenise.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    #print(seed_text)\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "remember how she\n"
     ]
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenise.word_index.items()))\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "pattern_val = [i for i in pattern if i>0]\n",
    "print(\"Seed:\")\n",
    "print(' '.join([reverse_word_map.get(value) for value in pattern_val]))\n",
    "seed_text = [reverse_word_map.get(value)+' ' for value in pattern_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/86/b58mx9y52mq_z0wkgsjtgjqh0000gn/T/ipykernel_1912/3231233077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_lyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/86/b58mx9y52mq_z0wkgsjtgjqh0000gn/T/ipykernel_1912/1259206048.py\u001b[0m in \u001b[0;36mmake_lyrics\u001b[0;34m(seed_text, next_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpred_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         token_list = pad_sequences([token_list],\n\u001b[1;32m      6\u001b[0m                      maxlen=max_sequence_len-1,padding='pre')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenise' is not defined"
     ]
    }
   ],
   "source": [
    "make_lyrics('I',40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "# saving\n",
    "#with open('tokenizer.pickle', 'wb') as handle:\n",
    "    #pickle.dump(tokenise, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:47:08.873548: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('../my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lyrics(seed_text, next_words):\n",
    "    pred_index=[]\n",
    "    for i in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=max_sequence_len-1,padding='pre')\n",
    "        #print(token_list.shape)\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len-1, 1))\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index =  np.argmax(predicted)\n",
    "        pred_index.append(predicted_index)\n",
    "        \n",
    "\n",
    "\n",
    "        #predicted_index=1\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    #print(seed_text)\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don’t know i was really drunk at the time is gone the song is over thought i’d something more had it doon by the haim ‘ma place well i slapped me and i slapped it doon in the side and'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_lyrics('I',40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95a9795d59b664a2d105a5ec7fd243dd9a0ea7d218927739a1488379bb5ae0b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
