{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../raw_data/pink_floyd_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'side': 1,\n",
       " 'that': 2,\n",
       " 'cat’s': 3,\n",
       " 'something': 4,\n",
       " 'i': 5,\n",
       " 'can’t': 6,\n",
       " 'explain': 7,\n",
       " 'cat': 8,\n",
       " 'you’re': 9,\n",
       " 'a': 10,\n",
       " 'the': 11,\n",
       " 'be': 12,\n",
       " 'lucifer': 13,\n",
       " 'always': 14,\n",
       " 'by': 15,\n",
       " 'your': 16,\n",
       " 'around': 17,\n",
       " 'sam': 18,\n",
       " 'siam': 19,\n",
       " 'sitting': 20,\n",
       " 'jennifer': 21,\n",
       " 'gentle': 22,\n",
       " 'witch': 23,\n",
       " 'left': 24,\n",
       " 'he’s': 25,\n",
       " 'right': 26,\n",
       " 'oh': 27,\n",
       " 'no': 28,\n",
       " 'go': 29,\n",
       " 'to': 30,\n",
       " 'sea': 31,\n",
       " 'hip': 32,\n",
       " 'ship’s': 33,\n",
       " 'somewhere': 34,\n",
       " 'anywhere': 35,\n",
       " 'at': 36,\n",
       " 'night': 37,\n",
       " 'prowling': 38,\n",
       " 'sifting': 39,\n",
       " 'sand': 40,\n",
       " 'hiding': 41,\n",
       " 'on': 42,\n",
       " 'ground': 43,\n",
       " 'he’ll': 44,\n",
       " 'found': 45,\n",
       " 'when': 46}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(char_level=False)\n",
    "tokenizer.fit_on_texts([df.lyrics.iloc[1].replace('\\n',' \\n ')])\n",
    "tokenizer.word_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.lyrics.iloc[1].split('\\n')\n",
    "text = [re.sub(r'\\d+', '', i) for i in text]\n",
    "corpus = list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'He’ll be found when you’re around',\n",
       " 'Hiding around on the ground',\n",
       " 'Lucifer Sam, siam cat',\n",
       " 'You’re the left side, he’s the right side',\n",
       " 'Be a hip cat, be a ship’s cat',\n",
       " 'That cat’s something I can’t explain',\n",
       " 'Always by your side',\n",
       " 'At night prowling, sifting sand',\n",
       " 'Oh, no!',\n",
       " 'Somewhere, anywhere',\n",
       " 'Always sitting by your side',\n",
       " 'Lucifer, go to sea',\n",
       " 'Jennifer Gentle, you’re a witch']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "[]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "[44, 12, 45, 46, 9, 17]\n",
      "--------------------------------------------------\n",
      "[44, 12]\n",
      "[44, 12, 45]\n",
      "[44, 12, 45, 46]\n",
      "[44, 12, 45, 46, 9]\n",
      "[44, 12, 45, 46, 9, 17]\n",
      "--------------------------------------------------\n",
      "[41, 17, 42, 11, 43]\n",
      "--------------------------------------------------\n",
      "[41, 17]\n",
      "[41, 17, 42]\n",
      "[41, 17, 42, 11]\n",
      "[41, 17, 42, 11, 43]\n",
      "--------------------------------------------------\n",
      "[13, 18, 19, 8]\n",
      "--------------------------------------------------\n",
      "[13, 18]\n",
      "[13, 18, 19]\n",
      "[13, 18, 19, 8]\n",
      "--------------------------------------------------\n",
      "[9, 11, 24, 1, 25, 11, 26, 1]\n",
      "--------------------------------------------------\n",
      "[9, 11]\n",
      "[9, 11, 24]\n",
      "[9, 11, 24, 1]\n",
      "[9, 11, 24, 1, 25]\n",
      "[9, 11, 24, 1, 25, 11]\n",
      "[9, 11, 24, 1, 25, 11, 26]\n",
      "[9, 11, 24, 1, 25, 11, 26, 1]\n",
      "--------------------------------------------------\n",
      "[12, 10, 32, 8, 12, 10, 33, 8]\n",
      "--------------------------------------------------\n",
      "[12, 10]\n",
      "[12, 10, 32]\n",
      "[12, 10, 32, 8]\n",
      "[12, 10, 32, 8, 12]\n",
      "[12, 10, 32, 8, 12, 10]\n",
      "[12, 10, 32, 8, 12, 10, 33]\n",
      "[12, 10, 32, 8, 12, 10, 33, 8]\n",
      "--------------------------------------------------\n",
      "[2, 3, 4, 5, 6, 7]\n",
      "--------------------------------------------------\n",
      "[2, 3]\n",
      "[2, 3, 4]\n",
      "[2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[2, 3, 4, 5, 6, 7]\n",
      "--------------------------------------------------\n",
      "[14, 15, 16, 1]\n",
      "--------------------------------------------------\n",
      "[14, 15]\n",
      "[14, 15, 16]\n",
      "[14, 15, 16, 1]\n",
      "--------------------------------------------------\n",
      "[36, 37, 38, 39, 40]\n",
      "--------------------------------------------------\n",
      "[36, 37]\n",
      "[36, 37, 38]\n",
      "[36, 37, 38, 39]\n",
      "[36, 37, 38, 39, 40]\n",
      "--------------------------------------------------\n",
      "[27, 28]\n",
      "--------------------------------------------------\n",
      "[27, 28]\n",
      "--------------------------------------------------\n",
      "[34, 35]\n",
      "--------------------------------------------------\n",
      "[34, 35]\n",
      "--------------------------------------------------\n",
      "[14, 20, 15, 16, 1]\n",
      "--------------------------------------------------\n",
      "[14, 20]\n",
      "[14, 20, 15]\n",
      "[14, 20, 15, 16]\n",
      "[14, 20, 15, 16, 1]\n",
      "--------------------------------------------------\n",
      "[13, 29, 30, 31]\n",
      "--------------------------------------------------\n",
      "[13, 29]\n",
      "[13, 29, 30]\n",
      "[13, 29, 30, 31]\n",
      "--------------------------------------------------\n",
      "[21, 22, 9, 10, 23]\n",
      "--------------------------------------------------\n",
      "[21, 22]\n",
      "[21, 22, 9]\n",
      "[21, 22, 9, 10]\n",
      "[21, 22, 9, 10, 23]\n"
     ]
    }
   ],
   "source": [
    "lines=[]\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    print('--'*25)\n",
    "    print(token_list)\n",
    "    print('--'*25)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        print(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(token_list):\n",
    "  ng = []\n",
    "  for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    ng.append(n_gram_sequence)\n",
    "  return ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Moon in both [houses]...\"...Scorpio, [Arabian Skies], Libra...\"...Pluto was not discovered until 1930...\"\\nLime and limpid green, a second scene\\nA fight between the blue you once knew\\nFloating down, the sound resounds\\nAround the icy waters underground\\nJupiter and Saturn, Oberon, Miranda and Titania\\nNeptune, Titan, stars can frighten\\n\\nBlinding signs flap\\nFlicker, flicker, flicker, blam\\nPow, pow\\nStairway scare Dan Dare who’s there?\\n\\nLime and limpid green, the sound surrounds\\nThe icy waters under\\nLime and limpid green, the sound surrounds\\nThe icy waters underground'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[0]].lyrics.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqform(data):\n",
    "  tokenise = Tokenizer()\n",
    "  input_sequences = []\n",
    "  corpus = []\n",
    "  k=0\n",
    "  for i in range(0,len(df)):\n",
    "      text = df.iloc[[i]].lyrics.iloc[0]\n",
    "      if type(text)==float:\n",
    "          pass\n",
    "      else:\n",
    "          text = text.lower().split(\"\\n\")\n",
    "          text = [re.sub(r'\\d+', '', i) for i in text]\n",
    "          text = list(set(text))\n",
    "          if text==' ':\n",
    "              pass\n",
    "          else:\n",
    "              corpus.extend(text)\n",
    "              k+=1\n",
    "  tokenise.fit_on_texts(corpus)\n",
    "  for line in corpus:\n",
    "      token_list = tokenise.texts_to_sequences([line])[0]\n",
    "      input_sequences.extend(ngram(token_list))\n",
    " \n",
    "  \n",
    "  max_sequence_len = max([len(x) for x in input_sequences])\n",
    "  input_sequences = np.array(pad_sequences(input_sequences,\n",
    "                       maxlen = max_sequence_len, padding='pre'))\n",
    "  \n",
    "  predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "  fin_data = pd.DataFrame(np.hstack((predictors, label.reshape(-1,1))),columns=np.hstack((np.arange(1,predictors.shape[1]+1),np.array(['label']))))\n",
    "  total_words = len(tokenise.word_index) + 1\n",
    "  print('{} number of lyrics inputted'.format(k))\n",
    "\n",
    "  return fin_data,tokenise,max_sequence_len,total_words,predictors,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 number of lyrics inputted\n",
      "(13839, 88) 88 2983\n"
     ]
    }
   ],
   "source": [
    "fdf,tokenise,max_sequence_len,total_words,predictors,label = seqform(df)\n",
    "print(fdf.shape,max_sequence_len,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf .to_csv('fin_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13839\n",
      "13839\n"
     ]
    }
   ],
   "source": [
    "dataX = [fdf.iloc[i,0:87].tolist() for i in range(0,fdf.shape[0])]\n",
    "dataY = [fdf.iloc[i,87] for i in range(0,fdf.shape[0])]\n",
    "print(len(dataX))\n",
    "print(len(dataY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (13839, 87, 1))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 10:17:42.644914: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 87, 150)           447450    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 87, 300)          361200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 87, 300)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1491)              150591    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2983)              4450636   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,570,277\n",
      "Trainable params: 5,570,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 150, input_length=max_sequence_len-1))\n",
    "# Add an LSTM Layer\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))  \n",
    "# A dropout layer for regularisation\n",
    "model.add(Dropout(0.2))\n",
    "# Add another LSTM Layer\n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(total_words/2, activation='relu'))  \n",
    "# In the last layer, the shape should be equal to the total number of words present in our corpus\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "#model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')  #(# Pick a loss function and an optimizer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 6.5845 - accuracy: 0.0654\n",
      "Epoch 00001: loss improved from inf to 6.58453, saving model to weights-improvement-01-6.5845.hdf5\n",
      "433/433 [==============================] - 211s 468ms/step - loss: 6.5845 - accuracy: 0.0654\n",
      "Epoch 2/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 6.0923 - accuracy: 0.0759\n",
      "Epoch 00002: loss improved from 6.58453 to 6.09227, saving model to weights-improvement-02-6.0923.hdf5\n",
      "433/433 [==============================] - 174s 403ms/step - loss: 6.0923 - accuracy: 0.0759\n",
      "Epoch 3/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 5.9019 - accuracy: 0.0778\n",
      "Epoch 00003: loss improved from 6.09227 to 5.90190, saving model to weights-improvement-03-5.9019.hdf5\n",
      "433/433 [==============================] - 205s 475ms/step - loss: 5.9019 - accuracy: 0.0778\n",
      "Epoch 4/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 5.7034 - accuracy: 0.0806\n",
      "Epoch 00004: loss improved from 5.90190 to 5.70344, saving model to weights-improvement-04-5.7034.hdf5\n",
      "433/433 [==============================] - 160s 371ms/step - loss: 5.7034 - accuracy: 0.0806\n",
      "Epoch 5/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 5.4823 - accuracy: 0.0899\n",
      "Epoch 00005: loss improved from 5.70344 to 5.48230, saving model to weights-improvement-05-5.4823.hdf5\n",
      "433/433 [==============================] - 173s 400ms/step - loss: 5.4823 - accuracy: 0.0899\n",
      "Epoch 6/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 5.2625 - accuracy: 0.1002\n",
      "Epoch 00006: loss improved from 5.48230 to 5.26254, saving model to weights-improvement-06-5.2625.hdf5\n",
      "433/433 [==============================] - 138s 318ms/step - loss: 5.2625 - accuracy: 0.1002\n",
      "Epoch 7/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 5.0776 - accuracy: 0.1132\n",
      "Epoch 00007: loss improved from 5.26254 to 5.07764, saving model to weights-improvement-07-5.0776.hdf5\n",
      "433/433 [==============================] - 137s 316ms/step - loss: 5.0776 - accuracy: 0.1132\n",
      "Epoch 8/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 4.8982 - accuracy: 0.1265\n",
      "Epoch 00008: loss improved from 5.07764 to 4.89822, saving model to weights-improvement-08-4.8982.hdf5\n",
      "433/433 [==============================] - 136s 314ms/step - loss: 4.8982 - accuracy: 0.1265\n",
      "Epoch 9/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 4.7154 - accuracy: 0.1350\n",
      "Epoch 00009: loss improved from 4.89822 to 4.71536, saving model to weights-improvement-09-4.7154.hdf5\n",
      "433/433 [==============================] - 142s 328ms/step - loss: 4.7154 - accuracy: 0.1350\n",
      "Epoch 10/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 4.5580 - accuracy: 0.1454\n",
      "Epoch 00010: loss improved from 4.71536 to 4.55800, saving model to weights-improvement-10-4.5580.hdf5\n",
      "433/433 [==============================] - 145s 335ms/step - loss: 4.5580 - accuracy: 0.1454\n",
      "Epoch 11/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 4.3850 - accuracy: 0.1550\n",
      "Epoch 00011: loss improved from 4.55800 to 4.38503, saving model to weights-improvement-11-4.3850.hdf5\n",
      "433/433 [==============================] - 158s 366ms/step - loss: 4.3850 - accuracy: 0.1550\n",
      "Epoch 12/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 4.2102 - accuracy: 0.1699\n",
      "Epoch 00012: loss improved from 4.38503 to 4.21022, saving model to weights-improvement-12-4.2102.hdf5\n",
      "433/433 [==============================] - 380s 879ms/step - loss: 4.2102 - accuracy: 0.1699\n",
      "Epoch 13/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 4.0449 - accuracy: 0.1799\n",
      "Epoch 00013: loss improved from 4.21022 to 4.04489, saving model to weights-improvement-13-4.0449.hdf5\n",
      "433/433 [==============================] - 235s 542ms/step - loss: 4.0449 - accuracy: 0.1799\n",
      "Epoch 14/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 3.8907 - accuracy: 0.1941\n",
      "Epoch 00014: loss improved from 4.04489 to 3.89073, saving model to weights-improvement-14-3.8907.hdf5\n",
      "433/433 [==============================] - 176s 406ms/step - loss: 3.8907 - accuracy: 0.1941\n",
      "Epoch 15/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 3.7199 - accuracy: 0.2080\n",
      "Epoch 00015: loss improved from 3.89073 to 3.71992, saving model to weights-improvement-15-3.7199.hdf5\n",
      "433/433 [==============================] - 192s 443ms/step - loss: 3.7199 - accuracy: 0.2080\n",
      "Epoch 16/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 3.5649 - accuracy: 0.2249\n",
      "Epoch 00016: loss improved from 3.71992 to 3.56494, saving model to weights-improvement-16-3.5649.hdf5\n",
      "433/433 [==============================] - 181s 419ms/step - loss: 3.5649 - accuracy: 0.2249\n",
      "Epoch 17/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 3.4073 - accuracy: 0.2429\n",
      "Epoch 00017: loss improved from 3.56494 to 3.40735, saving model to weights-improvement-17-3.4073.hdf5\n",
      "433/433 [==============================] - 166s 384ms/step - loss: 3.4073 - accuracy: 0.2429\n",
      "Epoch 18/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 3.2569 - accuracy: 0.2683\n",
      "Epoch 00018: loss improved from 3.40735 to 3.25694, saving model to weights-improvement-18-3.2569.hdf5\n",
      "433/433 [==============================] - 174s 401ms/step - loss: 3.2569 - accuracy: 0.2683\n",
      "Epoch 19/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 3.1053 - accuracy: 0.2870\n",
      "Epoch 00019: loss improved from 3.25694 to 3.10532, saving model to weights-improvement-19-3.1053.hdf5\n",
      "433/433 [==============================] - 153s 353ms/step - loss: 3.1053 - accuracy: 0.2870\n",
      "Epoch 20/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.9648 - accuracy: 0.3118\n",
      "Epoch 00020: loss improved from 3.10532 to 2.96475, saving model to weights-improvement-20-2.9648.hdf5\n",
      "433/433 [==============================] - 154s 355ms/step - loss: 2.9648 - accuracy: 0.3118\n",
      "Epoch 21/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.8256 - accuracy: 0.3346\n",
      "Epoch 00021: loss improved from 2.96475 to 2.82555, saving model to weights-improvement-21-2.8256.hdf5\n",
      "433/433 [==============================] - 155s 357ms/step - loss: 2.8256 - accuracy: 0.3346\n",
      "Epoch 22/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.6910 - accuracy: 0.3561\n",
      "Epoch 00022: loss improved from 2.82555 to 2.69102, saving model to weights-improvement-22-2.6910.hdf5\n",
      "433/433 [==============================] - 156s 360ms/step - loss: 2.6910 - accuracy: 0.3561\n",
      "Epoch 23/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.5692 - accuracy: 0.3829\n",
      "Epoch 00023: loss improved from 2.69102 to 2.56919, saving model to weights-improvement-23-2.5692.hdf5\n",
      "433/433 [==============================] - 169s 390ms/step - loss: 2.5692 - accuracy: 0.3829\n",
      "Epoch 24/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.4585 - accuracy: 0.4049\n",
      "Epoch 00024: loss improved from 2.56919 to 2.45849, saving model to weights-improvement-24-2.4585.hdf5\n",
      "433/433 [==============================] - 165s 382ms/step - loss: 2.4585 - accuracy: 0.4049\n",
      "Epoch 25/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.3328 - accuracy: 0.4318\n",
      "Epoch 00025: loss improved from 2.45849 to 2.33278, saving model to weights-improvement-25-2.3328.hdf5\n",
      "433/433 [==============================] - 167s 386ms/step - loss: 2.3328 - accuracy: 0.4318\n",
      "Epoch 26/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.3763 - accuracy: 0.4451\n",
      "Epoch 00026: loss did not improve from 2.33278\n",
      "433/433 [==============================] - 221s 511ms/step - loss: 2.3763 - accuracy: 0.4451\n",
      "Epoch 27/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.1542 - accuracy: 0.4677\n",
      "Epoch 00027: loss improved from 2.33278 to 2.15416, saving model to weights-improvement-27-2.1542.hdf5\n",
      "433/433 [==============================] - 226s 522ms/step - loss: 2.1542 - accuracy: 0.4677\n",
      "Epoch 28/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 2.0401 - accuracy: 0.4932\n",
      "Epoch 00028: loss improved from 2.15416 to 2.04008, saving model to weights-improvement-28-2.0401.hdf5\n",
      "433/433 [==============================] - 221s 512ms/step - loss: 2.0401 - accuracy: 0.4932\n",
      "Epoch 29/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.9606 - accuracy: 0.5096\n",
      "Epoch 00029: loss improved from 2.04008 to 1.96063, saving model to weights-improvement-29-1.9606.hdf5\n",
      "433/433 [==============================] - 209s 483ms/step - loss: 1.9606 - accuracy: 0.5096\n",
      "Epoch 30/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.8606 - accuracy: 0.5321\n",
      "Epoch 00030: loss improved from 1.96063 to 1.86061, saving model to weights-improvement-30-1.8606.hdf5\n",
      "433/433 [==============================] - 233s 537ms/step - loss: 1.8606 - accuracy: 0.5321\n",
      "Epoch 31/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.8042 - accuracy: 0.5391\n",
      "Epoch 00031: loss improved from 1.86061 to 1.80418, saving model to weights-improvement-31-1.8042.hdf5\n",
      "433/433 [==============================] - 167s 386ms/step - loss: 1.8042 - accuracy: 0.5391\n",
      "Epoch 32/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.7335 - accuracy: 0.5619\n",
      "Epoch 00032: loss improved from 1.80418 to 1.73354, saving model to weights-improvement-32-1.7335.hdf5\n",
      "433/433 [==============================] - 160s 370ms/step - loss: 1.7335 - accuracy: 0.5619\n",
      "Epoch 33/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.6595 - accuracy: 0.5755\n",
      "Epoch 00033: loss improved from 1.73354 to 1.65951, saving model to weights-improvement-33-1.6595.hdf5\n",
      "433/433 [==============================] - 172s 397ms/step - loss: 1.6595 - accuracy: 0.5755\n",
      "Epoch 34/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.5891 - accuracy: 0.5877\n",
      "Epoch 00034: loss improved from 1.65951 to 1.58912, saving model to weights-improvement-34-1.5891.hdf5\n",
      "433/433 [==============================] - 164s 378ms/step - loss: 1.5891 - accuracy: 0.5877\n",
      "Epoch 35/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.5188 - accuracy: 0.6100\n",
      "Epoch 00035: loss improved from 1.58912 to 1.51883, saving model to weights-improvement-35-1.5188.hdf5\n",
      "433/433 [==============================] - 166s 383ms/step - loss: 1.5188 - accuracy: 0.6100\n",
      "Epoch 36/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.4672 - accuracy: 0.6185\n",
      "Epoch 00036: loss improved from 1.51883 to 1.46718, saving model to weights-improvement-36-1.4672.hdf5\n",
      "433/433 [==============================] - 231s 535ms/step - loss: 1.4672 - accuracy: 0.6185\n",
      "Epoch 37/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.4162 - accuracy: 0.6274\n",
      "Epoch 00037: loss improved from 1.46718 to 1.41616, saving model to weights-improvement-37-1.4162.hdf5\n",
      "433/433 [==============================] - 209s 482ms/step - loss: 1.4162 - accuracy: 0.6274\n",
      "Epoch 38/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.3660 - accuracy: 0.6423\n",
      "Epoch 00038: loss improved from 1.41616 to 1.36600, saving model to weights-improvement-38-1.3660.hdf5\n",
      "433/433 [==============================] - 168s 389ms/step - loss: 1.3660 - accuracy: 0.6423\n",
      "Epoch 39/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.3724 - accuracy: 0.6355\n",
      "Epoch 00039: loss did not improve from 1.36600\n",
      "433/433 [==============================] - 177s 409ms/step - loss: 1.3724 - accuracy: 0.6355\n",
      "Epoch 40/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.3262 - accuracy: 0.6426\n",
      "Epoch 00040: loss improved from 1.36600 to 1.32615, saving model to weights-improvement-40-1.3262.hdf5\n",
      "433/433 [==============================] - 181s 418ms/step - loss: 1.3262 - accuracy: 0.6426\n",
      "Epoch 41/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.2327 - accuracy: 0.6705\n",
      "Epoch 00041: loss improved from 1.32615 to 1.23273, saving model to weights-improvement-41-1.2327.hdf5\n",
      "433/433 [==============================] - 175s 403ms/step - loss: 1.2327 - accuracy: 0.6705\n",
      "Epoch 42/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.1840 - accuracy: 0.6805\n",
      "Epoch 00042: loss improved from 1.23273 to 1.18396, saving model to weights-improvement-42-1.1840.hdf5\n",
      "433/433 [==============================] - 208s 480ms/step - loss: 1.1840 - accuracy: 0.6805\n",
      "Epoch 43/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.1460 - accuracy: 0.6923\n",
      "Epoch 00043: loss improved from 1.18396 to 1.14602, saving model to weights-improvement-43-1.1460.hdf5\n",
      "433/433 [==============================] - 165s 381ms/step - loss: 1.1460 - accuracy: 0.6923\n",
      "Epoch 44/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.1034 - accuracy: 0.6974\n",
      "Epoch 00044: loss improved from 1.14602 to 1.10341, saving model to weights-improvement-44-1.1034.hdf5\n",
      "433/433 [==============================] - 196s 452ms/step - loss: 1.1034 - accuracy: 0.6974\n",
      "Epoch 45/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.0887 - accuracy: 0.7004\n",
      "Epoch 00045: loss improved from 1.10341 to 1.08872, saving model to weights-improvement-45-1.0887.hdf5\n",
      "433/433 [==============================] - 214s 493ms/step - loss: 1.0887 - accuracy: 0.7004\n",
      "Epoch 46/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.0503 - accuracy: 0.7099\n",
      "Epoch 00046: loss improved from 1.08872 to 1.05028, saving model to weights-improvement-46-1.0503.hdf5\n",
      "433/433 [==============================] - 178s 411ms/step - loss: 1.0503 - accuracy: 0.7099\n",
      "Epoch 47/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 1.0132 - accuracy: 0.7193\n",
      "Epoch 00047: loss improved from 1.05028 to 1.01316, saving model to weights-improvement-47-1.0132.hdf5\n",
      "433/433 [==============================] - 156s 359ms/step - loss: 1.0132 - accuracy: 0.7193\n",
      "Epoch 48/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.9930 - accuracy: 0.7253\n",
      "Epoch 00048: loss improved from 1.01316 to 0.99303, saving model to weights-improvement-48-0.9930.hdf5\n",
      "433/433 [==============================] - 180s 416ms/step - loss: 0.9930 - accuracy: 0.7253\n",
      "Epoch 49/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.9831 - accuracy: 0.7258\n",
      "Epoch 00049: loss improved from 0.99303 to 0.98307, saving model to weights-improvement-49-0.9831.hdf5\n",
      "433/433 [==============================] - 156s 360ms/step - loss: 0.9831 - accuracy: 0.7258\n",
      "Epoch 50/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.9517 - accuracy: 0.7333\n",
      "Epoch 00050: loss improved from 0.98307 to 0.95171, saving model to weights-improvement-50-0.9517.hdf5\n",
      "433/433 [==============================] - 166s 383ms/step - loss: 0.9517 - accuracy: 0.7333\n",
      "Epoch 51/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.7373\n",
      "Epoch 00051: loss improved from 0.95171 to 0.93232, saving model to weights-improvement-51-0.9323.hdf5\n",
      "433/433 [==============================] - 211s 487ms/step - loss: 0.9323 - accuracy: 0.7373\n",
      "Epoch 52/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.9021 - accuracy: 0.7439\n",
      "Epoch 00052: loss improved from 0.93232 to 0.90210, saving model to weights-improvement-52-0.9021.hdf5\n",
      "433/433 [==============================] - 176s 407ms/step - loss: 0.9021 - accuracy: 0.7439\n",
      "Epoch 53/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.8770 - accuracy: 0.7558\n",
      "Epoch 00053: loss improved from 0.90210 to 0.87696, saving model to weights-improvement-53-0.8770.hdf5\n",
      "433/433 [==============================] - 188s 435ms/step - loss: 0.8770 - accuracy: 0.7558\n",
      "Epoch 54/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.8404 - accuracy: 0.7628\n",
      "Epoch 00054: loss improved from 0.87696 to 0.84039, saving model to weights-improvement-54-0.8404.hdf5\n",
      "433/433 [==============================] - 222s 513ms/step - loss: 0.8404 - accuracy: 0.7628\n",
      "Epoch 55/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.8351 - accuracy: 0.7627\n",
      "Epoch 00055: loss improved from 0.84039 to 0.83508, saving model to weights-improvement-55-0.8351.hdf5\n",
      "433/433 [==============================] - 203s 468ms/step - loss: 0.8351 - accuracy: 0.7627\n",
      "Epoch 56/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.8136 - accuracy: 0.7692\n",
      "Epoch 00056: loss improved from 0.83508 to 0.81364, saving model to weights-improvement-56-0.8136.hdf5\n",
      "433/433 [==============================] - 157s 363ms/step - loss: 0.8136 - accuracy: 0.7692\n",
      "Epoch 57/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.8120 - accuracy: 0.7670\n",
      "Epoch 00057: loss improved from 0.81364 to 0.81203, saving model to weights-improvement-57-0.8120.hdf5\n",
      "433/433 [==============================] - 158s 364ms/step - loss: 0.8120 - accuracy: 0.7670\n",
      "Epoch 58/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7820 - accuracy: 0.7753\n",
      "Epoch 00058: loss improved from 0.81203 to 0.78203, saving model to weights-improvement-58-0.7820.hdf5\n",
      "433/433 [==============================] - 160s 370ms/step - loss: 0.7820 - accuracy: 0.7753\n",
      "Epoch 59/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7552 - accuracy: 0.7821\n",
      "Epoch 00059: loss improved from 0.78203 to 0.75523, saving model to weights-improvement-59-0.7552.hdf5\n",
      "433/433 [==============================] - 159s 368ms/step - loss: 0.7552 - accuracy: 0.7821\n",
      "Epoch 60/60\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7534 - accuracy: 0.7803\n",
      "Epoch 00060: loss improved from 0.75523 to 0.75337, saving model to weights-improvement-60-0.7534.hdf5\n",
      "433/433 [==============================] - 159s 367ms/step - loss: 0.7534 - accuracy: 0.7803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c2a61f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs= 60,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7981 - accuracy: 0.7706\n",
      "Epoch 00001: loss did not improve from 0.75337\n",
      "433/433 [==============================] - 197s 436ms/step - loss: 0.7981 - accuracy: 0.7706\n",
      "Epoch 2/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7293 - accuracy: 0.7907\n",
      "Epoch 00002: loss improved from 0.75337 to 0.72934, saving model to weights-improvement-02-0.7293.hdf5\n",
      "433/433 [==============================] - 210s 484ms/step - loss: 0.7293 - accuracy: 0.7907\n",
      "Epoch 3/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7187 - accuracy: 0.7901\n",
      "Epoch 00003: loss improved from 0.72934 to 0.71867, saving model to weights-improvement-03-0.7187.hdf5\n",
      "433/433 [==============================] - 177s 408ms/step - loss: 0.7187 - accuracy: 0.7901\n",
      "Epoch 4/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7026 - accuracy: 0.7952\n",
      "Epoch 00004: loss improved from 0.71867 to 0.70256, saving model to weights-improvement-04-0.7026.hdf5\n",
      "433/433 [==============================] - 173s 400ms/step - loss: 0.7026 - accuracy: 0.7952\n",
      "Epoch 5/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.7904\n",
      "Epoch 00005: loss did not improve from 0.70256\n",
      "433/433 [==============================] - 171s 395ms/step - loss: 0.7123 - accuracy: 0.7904\n",
      "Epoch 6/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.7975\n",
      "Epoch 00006: loss improved from 0.70256 to 0.68408, saving model to weights-improvement-06-0.6841.hdf5\n",
      "433/433 [==============================] - 174s 402ms/step - loss: 0.6841 - accuracy: 0.7975\n",
      "Epoch 7/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.7985\n",
      "Epoch 00007: loss improved from 0.68408 to 0.68271, saving model to weights-improvement-07-0.6827.hdf5\n",
      "433/433 [==============================] - 171s 395ms/step - loss: 0.6827 - accuracy: 0.7985\n",
      "Epoch 8/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6740 - accuracy: 0.8022\n",
      "Epoch 00008: loss improved from 0.68271 to 0.67403, saving model to weights-improvement-08-0.6740.hdf5\n",
      "433/433 [==============================] - 171s 396ms/step - loss: 0.6740 - accuracy: 0.8022\n",
      "Epoch 9/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6765 - accuracy: 0.8022\n",
      "Epoch 00009: loss did not improve from 0.67403\n",
      "433/433 [==============================] - 172s 397ms/step - loss: 0.6765 - accuracy: 0.8022\n",
      "Epoch 10/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6527 - accuracy: 0.8082\n",
      "Epoch 00010: loss improved from 0.67403 to 0.65268, saving model to weights-improvement-10-0.6527.hdf5\n",
      "433/433 [==============================] - 144s 332ms/step - loss: 0.6527 - accuracy: 0.8082\n",
      "Epoch 11/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6457 - accuracy: 0.8089\n",
      "Epoch 00011: loss improved from 0.65268 to 0.64566, saving model to weights-improvement-11-0.6457.hdf5\n",
      "433/433 [==============================] - 135s 313ms/step - loss: 0.6457 - accuracy: 0.8089\n",
      "Epoch 12/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6321 - accuracy: 0.8095\n",
      "Epoch 00012: loss improved from 0.64566 to 0.63211, saving model to weights-improvement-12-0.6321.hdf5\n",
      "433/433 [==============================] - 138s 319ms/step - loss: 0.6321 - accuracy: 0.8095\n",
      "Epoch 13/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.8045\n",
      "Epoch 00013: loss did not improve from 0.63211\n",
      "433/433 [==============================] - 137s 316ms/step - loss: 0.6610 - accuracy: 0.8045\n",
      "Epoch 14/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.8123\n",
      "Epoch 00014: loss improved from 0.63211 to 0.63170, saving model to weights-improvement-14-0.6317.hdf5\n",
      "433/433 [==============================] - 138s 319ms/step - loss: 0.6317 - accuracy: 0.8123\n",
      "Epoch 15/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.8123\n",
      "Epoch 00015: loss improved from 0.63170 to 0.62567, saving model to weights-improvement-15-0.6257.hdf5\n",
      "433/433 [==============================] - 139s 321ms/step - loss: 0.6257 - accuracy: 0.8123\n",
      "Epoch 16/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.8184\n",
      "Epoch 00016: loss improved from 0.62567 to 0.60186, saving model to weights-improvement-16-0.6019.hdf5\n",
      "433/433 [==============================] - 188s 435ms/step - loss: 0.6019 - accuracy: 0.8184\n",
      "Epoch 17/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.8204\n",
      "Epoch 00017: loss improved from 0.60186 to 0.59559, saving model to weights-improvement-17-0.5956.hdf5\n",
      "433/433 [==============================] - 173s 400ms/step - loss: 0.5956 - accuracy: 0.8204\n",
      "Epoch 18/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6018 - accuracy: 0.8210\n",
      "Epoch 00018: loss did not improve from 0.59559\n",
      "433/433 [==============================] - 177s 409ms/step - loss: 0.6018 - accuracy: 0.8210\n",
      "Epoch 19/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.5904 - accuracy: 0.8210\n",
      "Epoch 00019: loss improved from 0.59559 to 0.59045, saving model to weights-improvement-19-0.5904.hdf5\n",
      "433/433 [==============================] - 167s 385ms/step - loss: 0.5904 - accuracy: 0.8210\n",
      "Epoch 20/20\n",
      "433/433 [==============================] - ETA: 0s - loss: 0.6003 - accuracy: 0.8174\n",
      "Epoch 00020: loss did not improve from 0.59045\n",
      "433/433 [==============================] - 186s 430ms/step - loss: 0.6003 - accuracy: 0.8174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d3256d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"weights-improvement-60-0.7534.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics='accuracy')\n",
    "model.fit(X, y, epochs=20, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5')\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lyrics(seed_text, next_words):\n",
    "    pred_index=[]\n",
    "    for i in range(next_words):\n",
    "        token_list = tokenise.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=max_sequence_len-1,padding='pre')\n",
    "        #print(token_list.shape)\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_len-1, 1))\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index =  np.argmax(predicted)\n",
    "        pred_index.append(predicted_index)\n",
    "        \n",
    "\n",
    "\n",
    "        #predicted_index=1\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenise.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    #print(seed_text)\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "or stay home by the fire filled by\n"
     ]
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenise.word_index.items()))\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "pattern_val = [i for i in pattern if i>0]\n",
    "print(\"Seed:\")\n",
    "print(' '.join([reverse_word_map.get(value) for value in pattern_val]))\n",
    "seed_text = [reverse_word_map.get(value)+' ' for value in pattern_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don’t know i was really drunk at the time is gone the song is over thought i’d something more had it doon by the haim ‘ma place well i slapped me and i slapped it doon in the side and'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_lyrics('I',40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenise, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95a9795d59b664a2d105a5ec7fd243dd9a0ea7d218927739a1488379bb5ae0b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
